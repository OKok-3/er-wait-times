import re
import hashlib
import pandas as pd
from .base import BaseScraper
from pendulum import parse
from httpx import get
from airflow.providers.amazon.aws.hooks.s3 import S3Hook
from airflow.providers.postgres.hooks.postgres import PostgresHook


class LHSC(BaseScraper):
    def scrape(self, ts: str) -> dict[str, str]:
        pg = PostgresHook(postgres_conn_id="owt-pg")
        s3 = S3Hook(aws_conn_id="garage-s3")

        response = get(self.url)

        # Initialize variables
        status_code = response.status_code
        error = None
        file_name = None
        file_hash = None
        skip_downstream = False

        # Capture any HTTP errors
        try:
            response.raise_for_status()
        except Exception as e:
            print(f"Error scraping {self.url}: {e}")
            error = str(e)
            skip_downstream = True

        # Process the response if there are no errors
        if not error:
            file_hash = hashlib.sha256(response.text.encode(encoding="utf-8")).hexdigest()
            file_name = f"{self.name}/{self.dept}/{self._id}_scraper_v{self.version}_{ts}.html"
            s3.load_string(
                string_data=response.text,
                key=file_name,
                bucket_name="open-wait-times",
            )

            # Check if the website has changed since the last fetch
            last_fetch = pg.get_first(
                sql="SELECT file_hash FROM owt.fetch_logs WHERE hospital_id = %s ORDER BY ts DESC LIMIT 1",
                parameters=[self._id],
            )
            skip_downstream = (file_hash == last_fetch[0]) if last_fetch else skip_downstream

        # Insert the fetch log into the database
        pg.insert_rows(
            table="owt.fetch_logs",
            rows=[
                (
                    # Fetch log ID will be generated by the database
                    self._id,
                    ts,
                    status_code,
                    error,
                    file_hash,
                    file_name,
                ),
            ],
            target_fields=[
                "hospital_id",
                "ts",
                "status_code",
                "error",
                "file_hash",
                "file_name",
            ],
        )

        fetch_log_id = pg.get_first(
            sql="SELECT id FROM owt.fetch_logs WHERE hospital_id = %s AND ts = %s",
            parameters=[self._id, ts],
        )
        return {
            "file_name": file_name,
            "fetch_log_id": fetch_log_id,
            "skip_downstream": skip_downstream,
        }

    def parse(self, data: dict[str, str]) -> dict[str, any]:
        s3 = S3Hook(aws_conn_id="garage-s3")
        html = str(s3.read_key(key=data["file_name"], bucket_name="open-wait-times"))

        hospital_identifier = self._id.split("_")[1].upper()
        wait_time_re = (
            f"<!--Start:{hospital_identifier}WaitTimeValue-->(.*?)<!--End:{hospital_identifier}WaitTimeValue-->"
        )
        time_updated_re = (
            f"<!--Start:{hospital_identifier}WaitTimeUpdated-->(.*?)<!--End:{hospital_identifier}WaitTimeUpdated-->"
        )

        wait_time_match = re.search(wait_time_re, html)
        time_updated_match = re.search(time_updated_re, html)

        data["wait_duration"] = wait_time_match.group(1).strip() if wait_time_match else None
        data["update_ts"] = time_updated_match.group(1).strip() if time_updated_match else None

        return data

    def load_data(self, data: dict[str, any]) -> None:
        pg = PostgresHook(postgres_conn_id="owt-pg")

        # We won't do None/Null checks and will rely on pipeline failing to catch this
        hospital_id = self._id
        fetch_log_id = data["fetch_log_id"]
        update_ts = parse(data["update_ts"], strict=False, tz="America/Toronto")

        wait_duration = pd.to_timedelta(data["wait_duration"].lower().strip())
        patient_arrival_time = update_ts.subtract(minutes=int(wait_duration.total_seconds() / 60))
        patient_departure_time = update_ts
        extra_info = None

        # TODO: Conflict will be dealt with by the to-be-implemented file hash checking step
        pg.insert_rows(
            table="owt.er_wait_times",
            rows=[
                (
                    hospital_id,
                    fetch_log_id,
                    update_ts,
                    wait_duration,
                    patient_arrival_time,
                    patient_departure_time,
                    extra_info,
                )
            ],
            target_fields=[
                "hospital_id",
                "fetch_log_id",
                "update_ts",
                "wait_duration",
                "patient_arrival_time",
                "patient_departure_time",
                "extra_info",
            ],
        )
